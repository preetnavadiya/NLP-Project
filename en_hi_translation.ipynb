{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/preetnavadiya/NLP-Project/blob/main/en_hi_translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers[sentencepiece] sacrebleu -q"
      ],
      "metadata": {
        "id": "qOsSkWNVW3Kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries"
      ],
      "metadata": {
        "id": "jz2CDShXW8Ds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import transformers\n",
        "import tensorflow as tf\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
        "from transformers import AdamWeightDecay\n",
        "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM"
      ],
      "metadata": {
        "id": "DziJgaL7W9Ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model checkpoint"
      ],
      "metadata": {
        "id": "24JojuRmXT2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"Helsinki-NLP/opus-mt-en-hi\"\n",
        "\n"
      ],
      "metadata": {
        "id": "9F-49LwSXVQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the dataset"
      ],
      "metadata": {
        "id": "HlAS0hv2Xf1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets = load_dataset(\"cfilt/iitb-english-hindi\")\n"
      ],
      "metadata": {
        "id": "yajgnsysXiFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets"
      ],
      "metadata": {
        "id": "zo9ncRpuX5yw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets['train'][5]"
      ],
      "metadata": {
        "id": "rO7kUUwiX-Kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer from pretrained model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "OaZI9tO_YU46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the tokenizer with a single sentence\n",
        "tokenizer(\"Hello, this is a sentence!\")"
      ],
      "metadata": {
        "id": "Ko8ht74IZB9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer([\"Hello, this is a sentence!\", \"This is another sentence.\"])\n"
      ],
      "metadata": {
        "id": "TCEuLAbAZSOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tokenizer.as_target_tokenizer():\n",
        "    print(tokenizer([\"एक संसाधनसंपन्न पहुँचनीयता अन्वेषक\"]))"
      ],
      "metadata": {
        "id": "84_YoEmqZ_2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set maximum sequence lengths\n",
        "max_input_length = 128\n",
        "max_target_length = 128\n",
        "\n",
        "# Define source and target languages\n",
        "source_lang = \"en\"\n",
        "target_lang = \"hi\"\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [ex[source_lang] for ex in examples[\"translation\"]]\n",
        "    targets = [ex[target_lang] for ex in examples[\"translation\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
        "\n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "UUUIqmSpaIoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the preprocessing function on a sample\n",
        "preprocess_function(raw_datasets[\"train\"][:2])"
      ],
      "metadata": {
        "id": "CnQNFDMHaLLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing to the entire dataset\n",
        "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n"
      ],
      "metadata": {
        "id": "aGPJ3Kq2agI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pretrained model\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n"
      ],
      "metadata": {
        "id": "d2MiI5htaqI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training hyperparameters\n",
        "batch_size = 16\n",
        "learning_rate = 2e-5\n",
        "weight_decay = 0.01\n",
        "num_train_epochs = 1\n"
      ],
      "metadata": {
        "id": "w9t3ss9ScXaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data collator for training\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")\n"
      ],
      "metadata": {
        "id": "1dsNDTqHcg_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data collator for generation/inference\n",
        "generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\", pad_to_multiple_of=8)"
      ],
      "metadata": {
        "id": "EbWVn6pxcpNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare training dataset\n",
        "train_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_datasets[\"train\"],\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "1I3cAYN5cuw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_datasets[\"validation\"],\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "nl_A_G8vc5xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare generation dataset (for inference/testing)\n",
        "generation_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_datasets[\"test\"],\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    collate_fn=generation_data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "nKgnptTDdEgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup optimizer with weight decay\n",
        "optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)\n",
        "model.compile(optimizer=optimizer)\n"
      ],
      "metadata": {
        "id": "bVsmKwA-dKSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(train_dataset, validation_data=validation_dataset, epochs=10)"
      ],
      "metadata": {
        "id": "FxAjihSEdQrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"tf_model/\")"
      ],
      "metadata": {
        "id": "IIV14VTHfmUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Testing"
      ],
      "metadata": {
        "id": "gFkIX40MfwuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload tokenizer and model for testing\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"tf_model/\")"
      ],
      "metadata": {
        "id": "ydUtY9tofwDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test translation\n",
        "input_text = \"Surprise! Motherfucker\"\n",
        "\n",
        "# Tokenize input text\n",
        "tokenized = tokenizer([input_text], return_tensors='np')\n",
        "\n",
        "# Generate translation\n",
        "out = model.generate(**tokenized, max_length=128)\n",
        "print(out)\n"
      ],
      "metadata": {
        "id": "H5i37ydKgAW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode the output to Hindi text\n",
        "with tokenizer.as_target_tokenizer():\n",
        "    print(tokenizer.decode(out[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "TWd1QxqVgeKK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}